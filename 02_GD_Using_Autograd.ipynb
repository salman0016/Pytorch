{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e24a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# f = w*x\n",
    "# f = 2*x\n",
    "\n",
    "x = np.array([1,2,3,4],dtype=np.float32)\n",
    "y = np.array([1,4,6,8],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd98b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2637769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fade93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "#loss\n",
    "def loss(y,y_predicted):\n",
    "    return((y_predicted-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96321e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient\n",
    "#MSE = 1/N*(w*x-y)**2\n",
    "#dj/dw = 1/N 2x(w*x-y)\n",
    "\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1506da4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training : f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before training : f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e930948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b943bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1f4969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-118.0\n",
      "epoch 1: w =1.180,loss=1.00000000\n",
      "-47.2\n",
      "epoch 2: w =1.652,loss=1.00000000\n",
      "-18.880001\n",
      "epoch 3: w =1.841,loss=1.00000000\n",
      "-7.5519986\n",
      "epoch 4: w =1.916,loss=1.00000000\n",
      "-3.0208013\n",
      "epoch 5: w =1.947,loss=1.00000000\n",
      "-1.2083225\n",
      "epoch 6: w =1.959,loss=1.00000000\n",
      "-0.48332644\n",
      "epoch 7: w =1.963,loss=1.00000000\n",
      "-0.19333315\n",
      "epoch 8: w =1.965,loss=1.00000000\n",
      "-0.07733154\n",
      "epoch 9: w =1.966,loss=1.00000000\n",
      "-0.030933619\n",
      "epoch 10: w =1.966,loss=1.00000000\n",
      "-0.012370586\n",
      "epoch 11: w =1.967,loss=1.00000000\n",
      "-0.0049476624\n",
      "epoch 12: w =1.967,loss=1.00000000\n",
      "-0.0019800663\n",
      "epoch 13: w =1.967,loss=1.00000000\n",
      "-0.00079131126\n",
      "epoch 14: w =1.967,loss=1.00000000\n",
      "-0.000320673\n",
      "epoch 15: w =1.967,loss=1.00000000\n",
      "-0.00012540817\n",
      "epoch 16: w =1.967,loss=1.00000000\n",
      "-4.7445297e-05\n",
      "epoch 17: w =1.967,loss=1.00000000\n",
      "-1.8835068e-05\n",
      "epoch 18: w =1.967,loss=1.00000000\n",
      "-1.0967255e-05\n",
      "epoch 19: w =1.967,loss=1.00000000\n",
      "-5.9604645e-06\n",
      "epoch 20: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 21: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 22: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 23: w =1.967,loss=1.00000000\n",
      "-5.9604645e-06\n",
      "epoch 24: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 25: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 26: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 27: w =1.967,loss=1.00000000\n",
      "-5.9604645e-06\n",
      "epoch 28: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 29: w =1.967,loss=1.00000000\n",
      "1.9073486e-06\n",
      "epoch 30: w =1.967,loss=1.00000000\n",
      "prediction after training:f(5)=9.833\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_iters):\n",
    "    #forward\n",
    "    y_pred = forward(x)\n",
    "    #Loss\n",
    "    l = loss(y,y_pred)\n",
    "    #gradient\n",
    "    dw = gradient(x,y,y_pred)\n",
    "    print(dw)\n",
    "    #update weights\n",
    "    w -= learning_rate*dw\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w ={w:.3f},loss={1:.8f}')\n",
    "print(f\"prediction after training:f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93199736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b7bfb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 887.4 MB 8.7 kB/s eta 0:00:010    |██████▏                         | 172.3 MB 3.2 MB/s eta 0:03:44     |████████▌                       | 236.7 MB 1.3 MB/s eta 0:08:25     |████████▉                       | 245.7 MB 2.2 MB/s eta 0:04:46     |███████████████                 | 415.6 MB 3.5 MB/s eta 0:02:15MB 1.0 MB/s eta 0:04:00��█████████████████       | 696.0 MB 19.0 MB/s eta 0:00:11     |█████████████████████████▉      | 715.7 MB 2.8 MB/s eta 0:01:02     |██████████████████████████▎     | 729.6 MB 2.3 MB/s eta 0:01:09     |█████████████████████████████▎  | 810.7 MB 3.4 MB/s eta 0:00:23█████████████████████████████▊  | 824.9 MB 1.2 MB/s eta 0:00:51\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/nvidia-cuda-nvrtc-cu11/\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/salman/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 7.2 kB/s eta 0:00:017    |████████                        | 139.8 MB 6.4 MB/s eta 0:01:05     |█████████████████████████▍      | 442.3 MB 695 kB/s eta 0:02:46 | 462.0 MB 480 kB/s eta 0:03:19     |███████████████████████████▊    | 481.9 MB 1.2 MB/s eta 0:01:04     |████████████████████████████▊   | 499.8 MB 4.2 MB/s eta 0:00:14     |█████████████████████████████▉  | 520.1 MB 7.5 MB/s eta 0:00:05     |██████████████████████████████▊ | 534.5 MB 3.6 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 16 kB/s  eta 0:00:011   |█▋                              | 16.4 MB 3.8 MB/s eta 0:01:19     |███▏                            | 31.6 MB 1.9 MB/s eta 0:02:33     |███▉                            | 37.8 MB 962 kB/s eta 0:04:51███                         | 69.6 MB 275 kB/s eta 0:14:58            | 69.8 MB 275 kB/s eta 0:14:58275 kB/s eta 0:14:57     |███████████████▏                | 150.1 MB 1.1 MB/s eta 0:02:27     |███████████████████▍            | 192.3 MB 1.2 MB/s eta 0:01:43   | 192.9 MB 1.2 MB/s eta 0:01:43     |█████████████████████           | 208.7 MB 712 kB/s eta 0:02:33��██████████████████████▎        | 231.1 MB 4.1 MB/s eta 0:00:21��█████████    | 278.5 MB 2.3 MB/s eta 0:00:17\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/salman/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.0)\n",
      "Requirement already satisfied: setuptools in /home/salman/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (58.0.4)\n",
      "Installing collected packages: nvidia-cublas-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "x = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y = torch.tensor([1,4,6,8],dtype=torch.float32)\n",
    "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3457240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training : f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "def forward(x):\n",
    "    return w*x\n",
    "def loss(y,y_predicted):\n",
    "    return((y_predicted-y)**2).mean()\n",
    "print(f\"prediction before training : f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c885f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "261f9357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =0.295,loss=29.25000000\n",
      "epoch 2: w =0.546,loss=21.20018768\n",
      "epoch 3: w =0.759,loss=15.38419724\n",
      "epoch 4: w =0.940,loss=11.18214512\n",
      "epoch 5: w =1.094,loss=8.14616203\n",
      "epoch 6: w =1.225,loss=5.95266438\n",
      "epoch 7: w =1.336,loss=4.36786270\n",
      "epoch 8: w =1.431,loss=3.22284269\n",
      "epoch 9: w =1.511,loss=2.39556646\n",
      "epoch 10: w =1.579,loss=1.79785860\n",
      "epoch 11: w =1.638,loss=1.36601555\n",
      "epoch 12: w =1.687,loss=1.05400896\n",
      "epoch 13: w =1.729,loss=0.82858384\n",
      "epoch 14: w =1.765,loss=0.66571426\n",
      "epoch 15: w =1.795,loss=0.54804134\n",
      "epoch 16: w =1.821,loss=0.46302229\n",
      "epoch 17: w =1.843,loss=0.40159622\n",
      "epoch 18: w =1.861,loss=0.35721567\n",
      "epoch 19: w =1.877,loss=0.32515085\n",
      "epoch 20: w =1.890,loss=0.30198389\n",
      "epoch 21: w =1.902,loss=0.28524587\n",
      "epoch 22: w =1.912,loss=0.27315259\n",
      "epoch 23: w =1.920,loss=0.26441529\n",
      "epoch 24: w =1.927,loss=0.25810257\n",
      "epoch 25: w =1.933,loss=0.25354165\n",
      "epoch 26: w =1.938,loss=0.25024632\n",
      "epoch 27: w =1.942,loss=0.24786550\n",
      "epoch 28: w =1.946,loss=0.24614532\n",
      "epoch 29: w =1.949,loss=0.24490248\n",
      "epoch 30: w =1.952,loss=0.24400455\n",
      "epoch 31: w =1.954,loss=0.24335578\n",
      "epoch 32: w =1.956,loss=0.24288708\n",
      "epoch 33: w =1.957,loss=0.24254841\n",
      "epoch 34: w =1.959,loss=0.24230374\n",
      "epoch 35: w =1.960,loss=0.24212694\n",
      "epoch 36: w =1.961,loss=0.24199921\n",
      "epoch 37: w =1.962,loss=0.24190693\n",
      "epoch 38: w =1.963,loss=0.24184027\n",
      "epoch 39: w =1.963,loss=0.24179210\n",
      "epoch 40: w =1.964,loss=0.24175730\n",
      "epoch 41: w =1.964,loss=0.24173214\n",
      "epoch 42: w =1.965,loss=0.24171396\n",
      "epoch 43: w =1.965,loss=0.24170083\n",
      "epoch 44: w =1.965,loss=0.24169137\n",
      "epoch 45: w =1.965,loss=0.24168451\n",
      "epoch 46: w =1.966,loss=0.24167953\n",
      "epoch 47: w =1.966,loss=0.24167597\n",
      "epoch 48: w =1.966,loss=0.24167341\n",
      "epoch 49: w =1.966,loss=0.24167153\n",
      "epoch 50: w =1.966,loss=0.24167019\n",
      "epoch 51: w =1.966,loss=0.24166919\n",
      "epoch 52: w =1.966,loss=0.24166849\n",
      "epoch 53: w =1.966,loss=0.24166799\n",
      "epoch 54: w =1.966,loss=0.24166763\n",
      "epoch 55: w =1.966,loss=0.24166735\n",
      "epoch 56: w =1.966,loss=0.24166717\n",
      "epoch 57: w =1.966,loss=0.24166702\n",
      "epoch 58: w =1.967,loss=0.24166693\n",
      "epoch 59: w =1.967,loss=0.24166687\n",
      "epoch 60: w =1.967,loss=0.24166679\n",
      "epoch 61: w =1.967,loss=0.24166678\n",
      "epoch 62: w =1.967,loss=0.24166675\n",
      "epoch 63: w =1.967,loss=0.24166672\n",
      "epoch 64: w =1.967,loss=0.24166670\n",
      "epoch 65: w =1.967,loss=0.24166670\n",
      "epoch 66: w =1.967,loss=0.24166667\n",
      "epoch 67: w =1.967,loss=0.24166667\n",
      "epoch 68: w =1.967,loss=0.24166669\n",
      "epoch 69: w =1.967,loss=0.24166666\n",
      "epoch 70: w =1.967,loss=0.24166667\n",
      "epoch 71: w =1.967,loss=0.24166670\n",
      "epoch 72: w =1.967,loss=0.24166666\n",
      "epoch 73: w =1.967,loss=0.24166667\n",
      "epoch 74: w =1.967,loss=0.24166669\n",
      "epoch 75: w =1.967,loss=0.24166666\n",
      "epoch 76: w =1.967,loss=0.24166667\n",
      "epoch 77: w =1.967,loss=0.24166667\n",
      "epoch 78: w =1.967,loss=0.24166666\n",
      "epoch 79: w =1.967,loss=0.24166666\n",
      "epoch 80: w =1.967,loss=0.24166666\n",
      "epoch 81: w =1.967,loss=0.24166667\n",
      "epoch 82: w =1.967,loss=0.24166666\n",
      "epoch 83: w =1.967,loss=0.24166666\n",
      "epoch 84: w =1.967,loss=0.24166667\n",
      "epoch 85: w =1.967,loss=0.24166666\n",
      "epoch 86: w =1.967,loss=0.24166666\n",
      "epoch 87: w =1.967,loss=0.24166667\n",
      "epoch 88: w =1.967,loss=0.24166667\n",
      "epoch 89: w =1.967,loss=0.24166666\n",
      "epoch 90: w =1.967,loss=0.24166667\n",
      "epoch 91: w =1.967,loss=0.24166666\n",
      "epoch 92: w =1.967,loss=0.24166666\n",
      "epoch 93: w =1.967,loss=0.24166667\n",
      "epoch 94: w =1.967,loss=0.24166667\n",
      "epoch 95: w =1.967,loss=0.24166667\n",
      "epoch 96: w =1.967,loss=0.24166666\n",
      "epoch 97: w =1.967,loss=0.24166666\n",
      "epoch 98: w =1.967,loss=0.24166666\n",
      "epoch 99: w =1.967,loss=0.24166666\n",
      "epoch 100: w =1.967,loss=0.24166666\n",
      "epoch 101: w =1.967,loss=0.24166666\n",
      "epoch 102: w =1.967,loss=0.24166666\n",
      "epoch 103: w =1.967,loss=0.24166666\n",
      "epoch 104: w =1.967,loss=0.24166666\n",
      "epoch 105: w =1.967,loss=0.24166666\n",
      "epoch 106: w =1.967,loss=0.24166666\n",
      "epoch 107: w =1.967,loss=0.24166666\n",
      "epoch 108: w =1.967,loss=0.24166666\n",
      "epoch 109: w =1.967,loss=0.24166666\n",
      "epoch 110: w =1.967,loss=0.24166666\n",
      "epoch 111: w =1.967,loss=0.24166666\n",
      "epoch 112: w =1.967,loss=0.24166666\n",
      "epoch 113: w =1.967,loss=0.24166666\n",
      "epoch 114: w =1.967,loss=0.24166666\n",
      "epoch 115: w =1.967,loss=0.24166666\n",
      "epoch 116: w =1.967,loss=0.24166666\n",
      "epoch 117: w =1.967,loss=0.24166666\n",
      "epoch 118: w =1.967,loss=0.24166666\n",
      "epoch 119: w =1.967,loss=0.24166666\n",
      "epoch 120: w =1.967,loss=0.24166666\n",
      "epoch 121: w =1.967,loss=0.24166666\n",
      "epoch 122: w =1.967,loss=0.24166666\n",
      "epoch 123: w =1.967,loss=0.24166666\n",
      "epoch 124: w =1.967,loss=0.24166666\n",
      "epoch 125: w =1.967,loss=0.24166666\n",
      "epoch 126: w =1.967,loss=0.24166666\n",
      "epoch 127: w =1.967,loss=0.24166666\n",
      "epoch 128: w =1.967,loss=0.24166666\n",
      "epoch 129: w =1.967,loss=0.24166666\n",
      "epoch 130: w =1.967,loss=0.24166666\n",
      "epoch 131: w =1.967,loss=0.24166666\n",
      "epoch 132: w =1.967,loss=0.24166666\n",
      "epoch 133: w =1.967,loss=0.24166666\n",
      "epoch 134: w =1.967,loss=0.24166666\n",
      "epoch 135: w =1.967,loss=0.24166666\n",
      "epoch 136: w =1.967,loss=0.24166666\n",
      "epoch 137: w =1.967,loss=0.24166666\n",
      "epoch 138: w =1.967,loss=0.24166666\n",
      "epoch 139: w =1.967,loss=0.24166666\n",
      "epoch 140: w =1.967,loss=0.24166666\n",
      "epoch 141: w =1.967,loss=0.24166666\n",
      "epoch 142: w =1.967,loss=0.24166666\n",
      "epoch 143: w =1.967,loss=0.24166666\n",
      "epoch 144: w =1.967,loss=0.24166666\n",
      "epoch 145: w =1.967,loss=0.24166666\n",
      "epoch 146: w =1.967,loss=0.24166666\n",
      "epoch 147: w =1.967,loss=0.24166666\n",
      "epoch 148: w =1.967,loss=0.24166666\n",
      "epoch 149: w =1.967,loss=0.24166666\n",
      "epoch 150: w =1.967,loss=0.24166666\n",
      "epoch 151: w =1.967,loss=0.24166666\n",
      "epoch 152: w =1.967,loss=0.24166666\n",
      "epoch 153: w =1.967,loss=0.24166666\n",
      "epoch 154: w =1.967,loss=0.24166666\n",
      "epoch 155: w =1.967,loss=0.24166666\n",
      "epoch 156: w =1.967,loss=0.24166666\n",
      "prediction after training:f(5)=9.833\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(x)\n",
    "    l = loss(y,y_pred)\n",
    "    #backward\n",
    "    l.backward()\n",
    "    #update weights\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate*w.grad\n",
    "    #zero grad\n",
    "    w.grad.zero_()\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w ={w:.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c55e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd907769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87a4bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y = torch.tensor([1,4,6,8],dtype=torch.float32)\n",
    "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1802590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25ef6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=0.000\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before Training : f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e000d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05b35018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =0.295,loss=29.25000000\n",
      "epoch 2: w =0.546,loss=21.20018768\n",
      "epoch 3: w =0.759,loss=15.38419914\n",
      "epoch 4: w =0.940,loss=11.18214607\n",
      "epoch 5: w =1.094,loss=8.14616203\n",
      "epoch 6: w =1.225,loss=5.95266438\n",
      "epoch 7: w =1.336,loss=4.36786270\n",
      "epoch 8: w =1.431,loss=3.22284269\n",
      "epoch 9: w =1.511,loss=2.39556646\n",
      "epoch 10: w =1.579,loss=1.79785860\n",
      "epoch 11: w =1.638,loss=1.36601555\n",
      "epoch 12: w =1.687,loss=1.05400896\n",
      "epoch 13: w =1.729,loss=0.82858384\n",
      "epoch 14: w =1.765,loss=0.66571426\n",
      "epoch 15: w =1.795,loss=0.54804134\n",
      "epoch 16: w =1.821,loss=0.46302229\n",
      "epoch 17: w =1.843,loss=0.40159622\n",
      "epoch 18: w =1.861,loss=0.35721567\n",
      "epoch 19: w =1.877,loss=0.32515085\n",
      "epoch 20: w =1.890,loss=0.30198389\n",
      "epoch 21: w =1.902,loss=0.28524587\n",
      "epoch 22: w =1.912,loss=0.27315259\n",
      "epoch 23: w =1.920,loss=0.26441529\n",
      "epoch 24: w =1.927,loss=0.25810257\n",
      "epoch 25: w =1.933,loss=0.25354165\n",
      "epoch 26: w =1.938,loss=0.25024632\n",
      "epoch 27: w =1.942,loss=0.24786550\n",
      "epoch 28: w =1.946,loss=0.24614532\n",
      "epoch 29: w =1.949,loss=0.24490248\n",
      "epoch 30: w =1.952,loss=0.24400455\n",
      "epoch 31: w =1.954,loss=0.24335578\n",
      "epoch 32: w =1.956,loss=0.24288708\n",
      "epoch 33: w =1.957,loss=0.24254841\n",
      "epoch 34: w =1.959,loss=0.24230374\n",
      "epoch 35: w =1.960,loss=0.24212694\n",
      "epoch 36: w =1.961,loss=0.24199921\n",
      "epoch 37: w =1.962,loss=0.24190693\n",
      "epoch 38: w =1.963,loss=0.24184027\n",
      "epoch 39: w =1.963,loss=0.24179210\n",
      "epoch 40: w =1.964,loss=0.24175730\n",
      "epoch 41: w =1.964,loss=0.24173214\n",
      "epoch 42: w =1.965,loss=0.24171396\n",
      "epoch 43: w =1.965,loss=0.24170083\n",
      "epoch 44: w =1.965,loss=0.24169137\n",
      "epoch 45: w =1.965,loss=0.24168451\n",
      "epoch 46: w =1.966,loss=0.24167953\n",
      "epoch 47: w =1.966,loss=0.24167597\n",
      "epoch 48: w =1.966,loss=0.24167341\n",
      "epoch 49: w =1.966,loss=0.24167153\n",
      "epoch 50: w =1.966,loss=0.24167019\n",
      "epoch 51: w =1.966,loss=0.24166919\n",
      "epoch 52: w =1.966,loss=0.24166849\n",
      "epoch 53: w =1.966,loss=0.24166799\n",
      "epoch 54: w =1.966,loss=0.24166763\n",
      "epoch 55: w =1.966,loss=0.24166735\n",
      "epoch 56: w =1.966,loss=0.24166717\n",
      "epoch 57: w =1.966,loss=0.24166702\n",
      "epoch 58: w =1.967,loss=0.24166693\n",
      "epoch 59: w =1.967,loss=0.24166687\n",
      "epoch 60: w =1.967,loss=0.24166679\n",
      "epoch 61: w =1.967,loss=0.24166678\n",
      "epoch 62: w =1.967,loss=0.24166675\n",
      "epoch 63: w =1.967,loss=0.24166672\n",
      "epoch 64: w =1.967,loss=0.24166670\n",
      "epoch 65: w =1.967,loss=0.24166670\n",
      "epoch 66: w =1.967,loss=0.24166667\n",
      "epoch 67: w =1.967,loss=0.24166667\n",
      "epoch 68: w =1.967,loss=0.24166669\n",
      "epoch 69: w =1.967,loss=0.24166666\n",
      "epoch 70: w =1.967,loss=0.24166667\n",
      "epoch 71: w =1.967,loss=0.24166670\n",
      "epoch 72: w =1.967,loss=0.24166666\n",
      "epoch 73: w =1.967,loss=0.24166667\n",
      "epoch 74: w =1.967,loss=0.24166669\n",
      "epoch 75: w =1.967,loss=0.24166666\n",
      "epoch 76: w =1.967,loss=0.24166667\n",
      "epoch 77: w =1.967,loss=0.24166667\n",
      "epoch 78: w =1.967,loss=0.24166666\n",
      "epoch 79: w =1.967,loss=0.24166666\n",
      "epoch 80: w =1.967,loss=0.24166666\n",
      "epoch 81: w =1.967,loss=0.24166667\n",
      "epoch 82: w =1.967,loss=0.24166666\n",
      "epoch 83: w =1.967,loss=0.24166666\n",
      "epoch 84: w =1.967,loss=0.24166667\n",
      "epoch 85: w =1.967,loss=0.24166666\n",
      "epoch 86: w =1.967,loss=0.24166666\n",
      "epoch 87: w =1.967,loss=0.24166667\n",
      "epoch 88: w =1.967,loss=0.24166667\n",
      "epoch 89: w =1.967,loss=0.24166666\n",
      "epoch 90: w =1.967,loss=0.24166667\n",
      "epoch 91: w =1.967,loss=0.24166666\n",
      "epoch 92: w =1.967,loss=0.24166666\n",
      "epoch 93: w =1.967,loss=0.24166667\n",
      "epoch 94: w =1.967,loss=0.24166667\n",
      "epoch 95: w =1.967,loss=0.24166667\n",
      "epoch 96: w =1.967,loss=0.24166666\n",
      "epoch 97: w =1.967,loss=0.24166666\n",
      "epoch 98: w =1.967,loss=0.24166666\n",
      "epoch 99: w =1.967,loss=0.24166666\n",
      "epoch 100: w =1.967,loss=0.24166666\n",
      "prediction after training:f(5)=9.833\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(x)\n",
    "    l = loss(y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch%1 == 0:\n",
    "        print(f'epoch {epoch+1}: w ={w:.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={forward(5):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89d68569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4th way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca71febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y = torch.tensor([[1],[4],[6],[8]],dtype=torch.float32)\n",
    "x_test = torch.tensor([5],dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "160c269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61086ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_features = x.shape\n",
    "print(n_samples)\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e22e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6ca87bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "907a3a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training : f(5)=2.381\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before training : f(5)={model(x_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bda28285",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04614d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =0.572,loss=1.00000000\n",
      "epoch 2: w =0.749,loss=1.00000000\n",
      "epoch 3: w =0.897,loss=1.00000000\n",
      "epoch 4: w =1.020,loss=1.00000000\n",
      "epoch 5: w =1.123,loss=1.00000000\n",
      "epoch 6: w =1.209,loss=1.00000000\n",
      "epoch 7: w =1.282,loss=1.00000000\n",
      "epoch 8: w =1.342,loss=1.00000000\n",
      "epoch 9: w =1.393,loss=1.00000000\n",
      "epoch 10: w =1.435,loss=1.00000000\n",
      "epoch 11: w =1.471,loss=1.00000000\n",
      "epoch 12: w =1.501,loss=1.00000000\n",
      "epoch 13: w =1.526,loss=1.00000000\n",
      "epoch 14: w =1.548,loss=1.00000000\n",
      "epoch 15: w =1.566,loss=1.00000000\n",
      "epoch 16: w =1.582,loss=1.00000000\n",
      "epoch 17: w =1.595,loss=1.00000000\n",
      "epoch 18: w =1.606,loss=1.00000000\n",
      "epoch 19: w =1.616,loss=1.00000000\n",
      "epoch 20: w =1.624,loss=1.00000000\n",
      "epoch 21: w =1.632,loss=1.00000000\n",
      "epoch 22: w =1.638,loss=1.00000000\n",
      "epoch 23: w =1.644,loss=1.00000000\n",
      "epoch 24: w =1.649,loss=1.00000000\n",
      "epoch 25: w =1.653,loss=1.00000000\n",
      "epoch 26: w =1.658,loss=1.00000000\n",
      "epoch 27: w =1.661,loss=1.00000000\n",
      "epoch 28: w =1.665,loss=1.00000000\n",
      "epoch 29: w =1.668,loss=1.00000000\n",
      "epoch 30: w =1.671,loss=1.00000000\n",
      "epoch 31: w =1.673,loss=1.00000000\n",
      "epoch 32: w =1.676,loss=1.00000000\n",
      "epoch 33: w =1.679,loss=1.00000000\n",
      "epoch 34: w =1.681,loss=1.00000000\n",
      "epoch 35: w =1.683,loss=1.00000000\n",
      "epoch 36: w =1.685,loss=1.00000000\n",
      "epoch 37: w =1.687,loss=1.00000000\n",
      "epoch 38: w =1.690,loss=1.00000000\n",
      "epoch 39: w =1.692,loss=1.00000000\n",
      "epoch 40: w =1.694,loss=1.00000000\n",
      "epoch 41: w =1.696,loss=1.00000000\n",
      "epoch 42: w =1.697,loss=1.00000000\n",
      "epoch 43: w =1.699,loss=1.00000000\n",
      "epoch 44: w =1.701,loss=1.00000000\n",
      "epoch 45: w =1.703,loss=1.00000000\n",
      "epoch 46: w =1.705,loss=1.00000000\n",
      "epoch 47: w =1.707,loss=1.00000000\n",
      "epoch 48: w =1.709,loss=1.00000000\n",
      "epoch 49: w =1.710,loss=1.00000000\n",
      "epoch 50: w =1.712,loss=1.00000000\n",
      "epoch 51: w =1.714,loss=1.00000000\n",
      "epoch 52: w =1.716,loss=1.00000000\n",
      "epoch 53: w =1.717,loss=1.00000000\n",
      "epoch 54: w =1.719,loss=1.00000000\n",
      "epoch 55: w =1.721,loss=1.00000000\n",
      "epoch 56: w =1.723,loss=1.00000000\n",
      "epoch 57: w =1.724,loss=1.00000000\n",
      "epoch 58: w =1.726,loss=1.00000000\n",
      "epoch 59: w =1.728,loss=1.00000000\n",
      "epoch 60: w =1.730,loss=1.00000000\n",
      "epoch 61: w =1.731,loss=1.00000000\n",
      "epoch 62: w =1.733,loss=1.00000000\n",
      "epoch 63: w =1.735,loss=1.00000000\n",
      "epoch 64: w =1.736,loss=1.00000000\n",
      "epoch 65: w =1.738,loss=1.00000000\n",
      "epoch 66: w =1.740,loss=1.00000000\n",
      "epoch 67: w =1.741,loss=1.00000000\n",
      "epoch 68: w =1.743,loss=1.00000000\n",
      "epoch 69: w =1.745,loss=1.00000000\n",
      "epoch 70: w =1.746,loss=1.00000000\n",
      "epoch 71: w =1.748,loss=1.00000000\n",
      "epoch 72: w =1.750,loss=1.00000000\n",
      "epoch 73: w =1.751,loss=1.00000000\n",
      "epoch 74: w =1.753,loss=1.00000000\n",
      "epoch 75: w =1.755,loss=1.00000000\n",
      "epoch 76: w =1.756,loss=1.00000000\n",
      "epoch 77: w =1.758,loss=1.00000000\n",
      "epoch 78: w =1.760,loss=1.00000000\n",
      "epoch 79: w =1.761,loss=1.00000000\n",
      "epoch 80: w =1.763,loss=1.00000000\n",
      "epoch 81: w =1.764,loss=1.00000000\n",
      "epoch 82: w =1.766,loss=1.00000000\n",
      "epoch 83: w =1.768,loss=1.00000000\n",
      "epoch 84: w =1.769,loss=1.00000000\n",
      "epoch 85: w =1.771,loss=1.00000000\n",
      "epoch 86: w =1.772,loss=1.00000000\n",
      "epoch 87: w =1.774,loss=1.00000000\n",
      "epoch 88: w =1.776,loss=1.00000000\n",
      "epoch 89: w =1.777,loss=1.00000000\n",
      "epoch 90: w =1.779,loss=1.00000000\n",
      "epoch 91: w =1.780,loss=1.00000000\n",
      "epoch 92: w =1.782,loss=1.00000000\n",
      "epoch 93: w =1.783,loss=1.00000000\n",
      "epoch 94: w =1.785,loss=1.00000000\n",
      "epoch 95: w =1.786,loss=1.00000000\n",
      "epoch 96: w =1.788,loss=1.00000000\n",
      "epoch 97: w =1.790,loss=1.00000000\n",
      "epoch 98: w =1.791,loss=1.00000000\n",
      "epoch 99: w =1.793,loss=1.00000000\n",
      "epoch 100: w =1.794,loss=1.00000000\n",
      "prediction after training:f(5)=9.458\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = model(x)\n",
    "    l = loss(y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch%1 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w ={w[0][0].item():.3f},loss={1:.8f}')\n",
    "print(f\"prediction after training:f(5)={model(x_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c9f3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "767a21e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y = torch.tensor([[1],[4],[6],[8]],dtype=torch.float32)\n",
    "X_test = torch.tensor([5],dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples)\n",
    "print(n_features)\n",
    "\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "598eac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.lin = nn.Linear(input_dim,output_dim)\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b96e8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(input_size,output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5645d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before Training : f(5)=-3.435\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction before Training : f(5)={model(X_test).item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7125c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ddce030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w =-0.202,loss=55.06926346\n",
      "epoch 2: w =0.137,loss=38.33658600\n",
      "epoch 3: w =0.419,loss=26.72551918\n",
      "epoch 4: w =0.654,loss=18.66824150\n",
      "epoch 5: w =0.850,loss=13.07686234\n",
      "epoch 6: w =1.014,loss=9.19652176\n",
      "epoch 7: w =1.150,loss=6.50343990\n",
      "epoch 8: w =1.264,loss=4.63417339\n",
      "epoch 9: w =1.360,loss=3.33653975\n",
      "epoch 10: w =1.439,loss=2.43555260\n",
      "epoch 11: w =1.506,loss=1.80979466\n",
      "epoch 12: w =1.561,loss=1.37501478\n",
      "epoch 13: w =1.608,loss=1.07275462\n",
      "epoch 14: w =1.647,loss=0.86245072\n",
      "epoch 15: w =1.679,loss=0.71595651\n",
      "epoch 16: w =1.707,loss=0.61374211\n",
      "epoch 17: w =1.730,loss=0.54225558\n",
      "epoch 18: w =1.749,loss=0.49209383\n",
      "epoch 19: w =1.766,loss=0.45673227\n",
      "epoch 20: w =1.780,loss=0.43164369\n",
      "epoch 21: w =1.792,loss=0.41368693\n",
      "epoch 22: w =1.802,loss=0.40068135\n",
      "epoch 23: w =1.810,loss=0.39111501\n",
      "epoch 24: w =1.818,loss=0.38393843\n",
      "epoch 25: w =1.824,loss=0.37842304\n",
      "epoch 26: w =1.830,loss=0.37406334\n",
      "epoch 27: w =1.835,loss=0.37050912\n",
      "epoch 28: w =1.839,loss=0.36751673\n",
      "epoch 29: w =1.843,loss=0.36491743\n",
      "epoch 30: w =1.846,loss=0.36259410\n",
      "epoch 31: w =1.849,loss=0.36046514\n",
      "epoch 32: w =1.852,loss=0.35847434\n",
      "epoch 33: w =1.854,loss=0.35658240\n",
      "epoch 34: w =1.857,loss=0.35476202\n",
      "epoch 35: w =1.859,loss=0.35299447\n",
      "epoch 36: w =1.861,loss=0.35126650\n",
      "epoch 37: w =1.862,loss=0.34956917\n",
      "epoch 38: w =1.864,loss=0.34789592\n",
      "epoch 39: w =1.866,loss=0.34624225\n",
      "epoch 40: w =1.868,loss=0.34460533\n",
      "epoch 41: w =1.869,loss=0.34298283\n",
      "epoch 42: w =1.871,loss=0.34137318\n",
      "epoch 43: w =1.872,loss=0.33977541\n",
      "epoch 44: w =1.874,loss=0.33818892\n",
      "epoch 45: w =1.875,loss=0.33661285\n",
      "epoch 46: w =1.876,loss=0.33504701\n",
      "epoch 47: w =1.878,loss=0.33349109\n",
      "epoch 48: w =1.879,loss=0.33194467\n",
      "epoch 49: w =1.880,loss=0.33040795\n",
      "epoch 50: w =1.882,loss=0.32888049\n",
      "epoch 51: w =1.883,loss=0.32736233\n",
      "epoch 52: w =1.884,loss=0.32585323\n",
      "epoch 53: w =1.886,loss=0.32435325\n",
      "epoch 54: w =1.887,loss=0.32286239\n",
      "epoch 55: w =1.888,loss=0.32138041\n",
      "epoch 56: w =1.889,loss=0.31990725\n",
      "epoch 57: w =1.891,loss=0.31844291\n",
      "epoch 58: w =1.892,loss=0.31698745\n",
      "epoch 59: w =1.893,loss=0.31554064\n",
      "epoch 60: w =1.894,loss=0.31410244\n",
      "epoch 61: w =1.895,loss=0.31267285\n",
      "epoch 62: w =1.897,loss=0.31125194\n",
      "epoch 63: w =1.898,loss=0.30983928\n",
      "epoch 64: w =1.899,loss=0.30843529\n",
      "epoch 65: w =1.900,loss=0.30703959\n",
      "epoch 66: w =1.901,loss=0.30565223\n",
      "epoch 67: w =1.903,loss=0.30427322\n",
      "epoch 68: w =1.904,loss=0.30290240\n",
      "epoch 69: w =1.905,loss=0.30153990\n",
      "epoch 70: w =1.906,loss=0.30018541\n",
      "epoch 71: w =1.907,loss=0.29883906\n",
      "epoch 72: w =1.909,loss=0.29750082\n",
      "epoch 73: w =1.910,loss=0.29617038\n",
      "epoch 74: w =1.911,loss=0.29484814\n",
      "epoch 75: w =1.912,loss=0.29353371\n",
      "epoch 76: w =1.913,loss=0.29222712\n",
      "epoch 77: w =1.914,loss=0.29092827\n",
      "epoch 78: w =1.916,loss=0.28963736\n",
      "epoch 79: w =1.917,loss=0.28835404\n",
      "epoch 80: w =1.918,loss=0.28707838\n",
      "epoch 81: w =1.919,loss=0.28581044\n",
      "epoch 82: w =1.920,loss=0.28455001\n",
      "epoch 83: w =1.921,loss=0.28329724\n",
      "epoch 84: w =1.922,loss=0.28205189\n",
      "epoch 85: w =1.924,loss=0.28081390\n",
      "epoch 86: w =1.925,loss=0.27958336\n",
      "epoch 87: w =1.926,loss=0.27836022\n",
      "epoch 88: w =1.927,loss=0.27714431\n",
      "epoch 89: w =1.928,loss=0.27593565\n",
      "epoch 90: w =1.929,loss=0.27473438\n",
      "epoch 91: w =1.930,loss=0.27354014\n",
      "epoch 92: w =1.931,loss=0.27235317\n",
      "epoch 93: w =1.932,loss=0.27117321\n",
      "epoch 94: w =1.934,loss=0.27000031\n",
      "epoch 95: w =1.935,loss=0.26883441\n",
      "epoch 96: w =1.936,loss=0.26767552\n",
      "epoch 97: w =1.937,loss=0.26652354\n",
      "epoch 98: w =1.938,loss=0.26537848\n",
      "epoch 99: w =1.939,loss=0.26424021\n",
      "epoch 100: w =1.940,loss=0.26310882\n",
      "epoch 101: w =1.941,loss=0.26198411\n",
      "epoch 102: w =1.942,loss=0.26086617\n",
      "epoch 103: w =1.943,loss=0.25975484\n",
      "epoch 104: w =1.944,loss=0.25865027\n",
      "epoch 105: w =1.945,loss=0.25755233\n",
      "epoch 106: w =1.947,loss=0.25646076\n",
      "epoch 107: w =1.948,loss=0.25537589\n",
      "epoch 108: w =1.949,loss=0.25429747\n",
      "epoch 109: w =1.950,loss=0.25322551\n",
      "epoch 110: w =1.951,loss=0.25215992\n",
      "epoch 111: w =1.952,loss=0.25110063\n",
      "epoch 112: w =1.953,loss=0.25004771\n",
      "epoch 113: w =1.954,loss=0.24900120\n",
      "epoch 114: w =1.955,loss=0.24796088\n",
      "epoch 115: w =1.956,loss=0.24692680\n",
      "epoch 116: w =1.957,loss=0.24589883\n",
      "epoch 117: w =1.958,loss=0.24487704\n",
      "epoch 118: w =1.959,loss=0.24386142\n",
      "epoch 119: w =1.960,loss=0.24285184\n",
      "epoch 120: w =1.961,loss=0.24184825\n",
      "epoch 121: w =1.962,loss=0.24085072\n",
      "epoch 122: w =1.963,loss=0.23985907\n",
      "epoch 123: w =1.964,loss=0.23887342\n",
      "epoch 124: w =1.965,loss=0.23789361\n",
      "epoch 125: w =1.966,loss=0.23691972\n",
      "epoch 126: w =1.967,loss=0.23595166\n",
      "epoch 127: w =1.968,loss=0.23498935\n",
      "epoch 128: w =1.969,loss=0.23403278\n",
      "epoch 129: w =1.970,loss=0.23308192\n",
      "epoch 130: w =1.971,loss=0.23213676\n",
      "epoch 131: w =1.972,loss=0.23119737\n",
      "epoch 132: w =1.973,loss=0.23026349\n",
      "epoch 133: w =1.974,loss=0.22933507\n",
      "epoch 134: w =1.975,loss=0.22841239\n",
      "epoch 135: w =1.976,loss=0.22749515\n",
      "epoch 136: w =1.977,loss=0.22658344\n",
      "epoch 137: w =1.978,loss=0.22567710\n",
      "epoch 138: w =1.979,loss=0.22477630\n",
      "epoch 139: w =1.980,loss=0.22388072\n",
      "epoch 140: w =1.981,loss=0.22299066\n",
      "epoch 141: w =1.982,loss=0.22210586\n",
      "epoch 142: w =1.983,loss=0.22122626\n",
      "epoch 143: w =1.984,loss=0.22035198\n",
      "epoch 144: w =1.985,loss=0.21948300\n",
      "epoch 145: w =1.986,loss=0.21861912\n",
      "epoch 146: w =1.986,loss=0.21776049\n",
      "epoch 147: w =1.987,loss=0.21690692\n",
      "epoch 148: w =1.988,loss=0.21605851\n",
      "epoch 149: w =1.989,loss=0.21521512\n",
      "epoch 150: w =1.990,loss=0.21437679\n",
      "epoch 151: w =1.991,loss=0.21354346\n",
      "epoch 152: w =1.992,loss=0.21271515\n",
      "epoch 153: w =1.993,loss=0.21189173\n",
      "epoch 154: w =1.994,loss=0.21107337\n",
      "epoch 155: w =1.995,loss=0.21025975\n",
      "epoch 156: w =1.996,loss=0.20945109\n",
      "epoch 157: w =1.997,loss=0.20864721\n",
      "epoch 158: w =1.998,loss=0.20784815\n",
      "epoch 159: w =1.998,loss=0.20705387\n",
      "epoch 160: w =1.999,loss=0.20626429\n",
      "epoch 161: w =2.000,loss=0.20547955\n",
      "epoch 162: w =2.001,loss=0.20469944\n",
      "epoch 163: w =2.002,loss=0.20392397\n",
      "epoch 164: w =2.003,loss=0.20315319\n",
      "epoch 165: w =2.004,loss=0.20238696\n",
      "epoch 166: w =2.005,loss=0.20162530\n",
      "epoch 167: w =2.006,loss=0.20086822\n",
      "epoch 168: w =2.006,loss=0.20011570\n",
      "epoch 169: w =2.007,loss=0.19936766\n",
      "epoch 170: w =2.008,loss=0.19862401\n",
      "epoch 171: w =2.009,loss=0.19788495\n",
      "epoch 172: w =2.010,loss=0.19715023\n",
      "epoch 173: w =2.011,loss=0.19641989\n",
      "epoch 174: w =2.012,loss=0.19569390\n",
      "epoch 175: w =2.013,loss=0.19497232\n",
      "epoch 176: w =2.013,loss=0.19425501\n",
      "epoch 177: w =2.014,loss=0.19354202\n",
      "epoch 178: w =2.015,loss=0.19283332\n",
      "epoch 179: w =2.016,loss=0.19212873\n",
      "epoch 180: w =2.017,loss=0.19142847\n",
      "epoch 181: w =2.018,loss=0.19073237\n",
      "epoch 182: w =2.019,loss=0.19004038\n",
      "epoch 183: w =2.019,loss=0.18935260\n",
      "epoch 184: w =2.020,loss=0.18866895\n",
      "epoch 185: w =2.021,loss=0.18798935\n",
      "epoch 186: w =2.022,loss=0.18731380\n",
      "epoch 187: w =2.023,loss=0.18664224\n",
      "epoch 188: w =2.024,loss=0.18597475\n",
      "epoch 189: w =2.024,loss=0.18531126\n",
      "epoch 190: w =2.025,loss=0.18465172\n",
      "epoch 191: w =2.026,loss=0.18399613\n",
      "epoch 192: w =2.027,loss=0.18334451\n",
      "epoch 193: w =2.028,loss=0.18269673\n",
      "epoch 194: w =2.029,loss=0.18205284\n",
      "epoch 195: w =2.029,loss=0.18141279\n",
      "epoch 196: w =2.030,loss=0.18077654\n",
      "epoch 197: w =2.031,loss=0.18014410\n",
      "epoch 198: w =2.032,loss=0.17951547\n",
      "epoch 199: w =2.033,loss=0.17889062\n",
      "epoch 200: w =2.033,loss=0.17826946\n",
      "epoch 201: w =2.034,loss=0.17765202\n",
      "epoch 202: w =2.035,loss=0.17703824\n",
      "epoch 203: w =2.036,loss=0.17642823\n",
      "epoch 204: w =2.037,loss=0.17582181\n",
      "epoch 205: w =2.037,loss=0.17521898\n",
      "epoch 206: w =2.038,loss=0.17461981\n",
      "epoch 207: w =2.039,loss=0.17402416\n",
      "epoch 208: w =2.040,loss=0.17343210\n",
      "epoch 209: w =2.040,loss=0.17284364\n",
      "epoch 210: w =2.041,loss=0.17225863\n",
      "epoch 211: w =2.042,loss=0.17167711\n",
      "epoch 212: w =2.043,loss=0.17109910\n",
      "epoch 213: w =2.044,loss=0.17052452\n",
      "epoch 214: w =2.044,loss=0.16995341\n",
      "epoch 215: w =2.045,loss=0.16938570\n",
      "epoch 216: w =2.046,loss=0.16882132\n",
      "epoch 217: w =2.047,loss=0.16826044\n",
      "epoch 218: w =2.047,loss=0.16770285\n",
      "epoch 219: w =2.048,loss=0.16714858\n",
      "epoch 220: w =2.049,loss=0.16659765\n",
      "epoch 221: w =2.050,loss=0.16605005\n",
      "epoch 222: w =2.050,loss=0.16550559\n",
      "epoch 223: w =2.051,loss=0.16496447\n",
      "epoch 224: w =2.052,loss=0.16442665\n",
      "epoch 225: w =2.053,loss=0.16389194\n",
      "epoch 226: w =2.053,loss=0.16336049\n",
      "epoch 227: w =2.054,loss=0.16283220\n",
      "epoch 228: w =2.055,loss=0.16230705\n",
      "epoch 229: w =2.056,loss=0.16178501\n",
      "epoch 230: w =2.056,loss=0.16126613\n",
      "epoch 231: w =2.057,loss=0.16075037\n",
      "epoch 232: w =2.058,loss=0.16023774\n",
      "epoch 233: w =2.058,loss=0.15972809\n",
      "epoch 234: w =2.059,loss=0.15922154\n",
      "epoch 235: w =2.060,loss=0.15871800\n",
      "epoch 236: w =2.061,loss=0.15821746\n",
      "epoch 237: w =2.061,loss=0.15771990\n",
      "epoch 238: w =2.062,loss=0.15722534\n",
      "epoch 239: w =2.063,loss=0.15673374\n",
      "epoch 240: w =2.063,loss=0.15624507\n",
      "epoch 241: w =2.064,loss=0.15575930\n",
      "epoch 242: w =2.065,loss=0.15527646\n",
      "epoch 243: w =2.066,loss=0.15479653\n",
      "epoch 244: w =2.066,loss=0.15431944\n",
      "epoch 245: w =2.067,loss=0.15384516\n",
      "epoch 246: w =2.068,loss=0.15337375\n",
      "epoch 247: w =2.068,loss=0.15290520\n",
      "epoch 248: w =2.069,loss=0.15243937\n",
      "epoch 249: w =2.070,loss=0.15197645\n",
      "epoch 250: w =2.070,loss=0.15151617\n",
      "epoch 251: w =2.071,loss=0.15105870\n",
      "epoch 252: w =2.072,loss=0.15060394\n",
      "epoch 253: w =2.073,loss=0.15015192\n",
      "epoch 254: w =2.073,loss=0.14970259\n",
      "epoch 255: w =2.074,loss=0.14925595\n",
      "epoch 256: w =2.075,loss=0.14881201\n",
      "epoch 257: w =2.075,loss=0.14837071\n",
      "epoch 258: w =2.076,loss=0.14793205\n",
      "epoch 259: w =2.077,loss=0.14749601\n",
      "epoch 260: w =2.077,loss=0.14706254\n",
      "epoch 261: w =2.078,loss=0.14663167\n",
      "epoch 262: w =2.079,loss=0.14620338\n",
      "epoch 263: w =2.079,loss=0.14577770\n",
      "epoch 264: w =2.080,loss=0.14535451\n",
      "epoch 265: w =2.081,loss=0.14493391\n",
      "epoch 266: w =2.081,loss=0.14451575\n",
      "epoch 267: w =2.082,loss=0.14410011\n",
      "epoch 268: w =2.083,loss=0.14368705\n",
      "epoch 269: w =2.083,loss=0.14327638\n",
      "epoch 270: w =2.084,loss=0.14286816\n",
      "epoch 271: w =2.084,loss=0.14246234\n",
      "epoch 272: w =2.085,loss=0.14205904\n",
      "epoch 273: w =2.086,loss=0.14165810\n",
      "epoch 274: w =2.086,loss=0.14125958\n",
      "epoch 275: w =2.087,loss=0.14086340\n",
      "epoch 276: w =2.088,loss=0.14046960\n",
      "epoch 277: w =2.088,loss=0.14007817\n",
      "epoch 278: w =2.089,loss=0.13968910\n",
      "epoch 279: w =2.090,loss=0.13930234\n",
      "epoch 280: w =2.090,loss=0.13891786\n",
      "epoch 281: w =2.091,loss=0.13853569\n",
      "epoch 282: w =2.091,loss=0.13815586\n",
      "epoch 283: w =2.092,loss=0.13777828\n",
      "epoch 284: w =2.093,loss=0.13740294\n",
      "epoch 285: w =2.093,loss=0.13702980\n",
      "epoch 286: w =2.094,loss=0.13665898\n",
      "epoch 287: w =2.095,loss=0.13629036\n",
      "epoch 288: w =2.095,loss=0.13592389\n",
      "epoch 289: w =2.096,loss=0.13555960\n",
      "epoch 290: w =2.096,loss=0.13519755\n",
      "epoch 291: w =2.097,loss=0.13483766\n",
      "epoch 292: w =2.098,loss=0.13447991\n",
      "epoch 293: w =2.098,loss=0.13412423\n",
      "epoch 294: w =2.099,loss=0.13377070\n",
      "epoch 295: w =2.099,loss=0.13341938\n",
      "epoch 296: w =2.100,loss=0.13307005\n",
      "epoch 297: w =2.101,loss=0.13272291\n",
      "epoch 298: w =2.101,loss=0.13237777\n",
      "epoch 299: w =2.102,loss=0.13203472\n",
      "epoch 300: w =2.102,loss=0.13169368\n",
      "epoch 301: w =2.103,loss=0.13135476\n",
      "epoch 302: w =2.104,loss=0.13101785\n",
      "epoch 303: w =2.104,loss=0.13068293\n",
      "epoch 304: w =2.105,loss=0.13035001\n",
      "epoch 305: w =2.105,loss=0.13001907\n",
      "epoch 306: w =2.106,loss=0.12969013\n",
      "epoch 307: w =2.107,loss=0.12936312\n",
      "epoch 308: w =2.107,loss=0.12903807\n",
      "epoch 309: w =2.108,loss=0.12871502\n",
      "epoch 310: w =2.108,loss=0.12839389\n",
      "epoch 311: w =2.109,loss=0.12807465\n",
      "epoch 312: w =2.109,loss=0.12775733\n",
      "epoch 313: w =2.110,loss=0.12744187\n",
      "epoch 314: w =2.111,loss=0.12712833\n",
      "epoch 315: w =2.111,loss=0.12681670\n",
      "epoch 316: w =2.112,loss=0.12650688\n",
      "epoch 317: w =2.112,loss=0.12619892\n",
      "epoch 318: w =2.113,loss=0.12589280\n",
      "epoch 319: w =2.113,loss=0.12558848\n",
      "epoch 320: w =2.114,loss=0.12528604\n",
      "epoch 321: w =2.114,loss=0.12498539\n",
      "epoch 322: w =2.115,loss=0.12468656\n",
      "epoch 323: w =2.116,loss=0.12438949\n",
      "epoch 324: w =2.116,loss=0.12409420\n",
      "epoch 325: w =2.117,loss=0.12380064\n",
      "epoch 326: w =2.117,loss=0.12350884\n",
      "epoch 327: w =2.118,loss=0.12321887\n",
      "epoch 328: w =2.118,loss=0.12293057\n",
      "epoch 329: w =2.119,loss=0.12264396\n",
      "epoch 330: w =2.119,loss=0.12235913\n",
      "epoch 331: w =2.120,loss=0.12207594\n",
      "epoch 332: w =2.121,loss=0.12179455\n",
      "epoch 333: w =2.121,loss=0.12151471\n",
      "epoch 334: w =2.122,loss=0.12123667\n",
      "epoch 335: w =2.122,loss=0.12096021\n",
      "epoch 336: w =2.123,loss=0.12068541\n",
      "epoch 337: w =2.123,loss=0.12041224\n",
      "epoch 338: w =2.124,loss=0.12014075\n",
      "epoch 339: w =2.124,loss=0.11987083\n",
      "epoch 340: w =2.125,loss=0.11960260\n",
      "epoch 341: w =2.125,loss=0.11933591\n",
      "epoch 342: w =2.126,loss=0.11907084\n",
      "epoch 343: w =2.126,loss=0.11880735\n",
      "epoch 344: w =2.127,loss=0.11854541\n",
      "epoch 345: w =2.127,loss=0.11828505\n",
      "epoch 346: w =2.128,loss=0.11802626\n",
      "epoch 347: w =2.128,loss=0.11776901\n",
      "epoch 348: w =2.129,loss=0.11751337\n",
      "epoch 349: w =2.129,loss=0.11725917\n",
      "epoch 350: w =2.130,loss=0.11700648\n",
      "epoch 351: w =2.130,loss=0.11675532\n",
      "epoch 352: w =2.131,loss=0.11650568\n",
      "epoch 353: w =2.131,loss=0.11625752\n",
      "epoch 354: w =2.132,loss=0.11601087\n",
      "epoch 355: w =2.132,loss=0.11576568\n",
      "epoch 356: w =2.133,loss=0.11552193\n",
      "epoch 357: w =2.133,loss=0.11527969\n",
      "epoch 358: w =2.134,loss=0.11503883\n",
      "epoch 359: w =2.134,loss=0.11479945\n",
      "epoch 360: w =2.135,loss=0.11456148\n",
      "epoch 361: w =2.135,loss=0.11432499\n",
      "epoch 362: w =2.136,loss=0.11408985\n",
      "epoch 363: w =2.136,loss=0.11385611\n",
      "epoch 364: w =2.137,loss=0.11362381\n",
      "epoch 365: w =2.137,loss=0.11339290\n",
      "epoch 366: w =2.138,loss=0.11316340\n",
      "epoch 367: w =2.138,loss=0.11293516\n",
      "epoch 368: w =2.139,loss=0.11270833\n",
      "epoch 369: w =2.139,loss=0.11248294\n",
      "epoch 370: w =2.140,loss=0.11225878\n",
      "epoch 371: w =2.140,loss=0.11203603\n",
      "epoch 372: w =2.141,loss=0.11181460\n",
      "epoch 373: w =2.141,loss=0.11159450\n",
      "epoch 374: w =2.142,loss=0.11137569\n",
      "epoch 375: w =2.142,loss=0.11115823\n",
      "epoch 376: w =2.143,loss=0.11094206\n",
      "epoch 377: w =2.143,loss=0.11072720\n",
      "epoch 378: w =2.144,loss=0.11051356\n",
      "epoch 379: w =2.144,loss=0.11030125\n",
      "epoch 380: w =2.145,loss=0.11009013\n",
      "epoch 381: w =2.145,loss=0.10988036\n",
      "epoch 382: w =2.145,loss=0.10967186\n",
      "epoch 383: w =2.146,loss=0.10946449\n",
      "epoch 384: w =2.146,loss=0.10925847\n",
      "epoch 385: w =2.147,loss=0.10905363\n",
      "epoch 386: w =2.147,loss=0.10885002\n",
      "epoch 387: w =2.148,loss=0.10864765\n",
      "epoch 388: w =2.148,loss=0.10844645\n",
      "epoch 389: w =2.149,loss=0.10824651\n",
      "epoch 390: w =2.149,loss=0.10804770\n",
      "epoch 391: w =2.150,loss=0.10785009\n",
      "epoch 392: w =2.150,loss=0.10765375\n",
      "epoch 393: w =2.151,loss=0.10745849\n",
      "epoch 394: w =2.151,loss=0.10726442\n",
      "epoch 395: w =2.151,loss=0.10707155\n",
      "epoch 396: w =2.152,loss=0.10687977\n",
      "epoch 397: w =2.152,loss=0.10668916\n",
      "epoch 398: w =2.153,loss=0.10649968\n",
      "epoch 399: w =2.153,loss=0.10631140\n",
      "epoch 400: w =2.154,loss=0.10612420\n",
      "epoch 401: w =2.154,loss=0.10593805\n",
      "epoch 402: w =2.154,loss=0.10575315\n",
      "epoch 403: w =2.155,loss=0.10556924\n",
      "epoch 404: w =2.155,loss=0.10538650\n",
      "epoch 405: w =2.156,loss=0.10520480\n",
      "epoch 406: w =2.156,loss=0.10502419\n",
      "epoch 407: w =2.157,loss=0.10484469\n",
      "epoch 408: w =2.157,loss=0.10466626\n",
      "epoch 409: w =2.158,loss=0.10448892\n",
      "epoch 410: w =2.158,loss=0.10431261\n",
      "epoch 411: w =2.158,loss=0.10413732\n",
      "epoch 412: w =2.159,loss=0.10396312\n",
      "epoch 413: w =2.159,loss=0.10378996\n",
      "epoch 414: w =2.160,loss=0.10361780\n",
      "epoch 415: w =2.160,loss=0.10344672\n",
      "epoch 416: w =2.160,loss=0.10327668\n",
      "epoch 417: w =2.161,loss=0.10310758\n",
      "epoch 418: w =2.161,loss=0.10293955\n",
      "epoch 419: w =2.162,loss=0.10277252\n",
      "epoch 420: w =2.162,loss=0.10260642\n",
      "epoch 421: w =2.163,loss=0.10244141\n",
      "epoch 422: w =2.163,loss=0.10227734\n",
      "epoch 423: w =2.163,loss=0.10211424\n",
      "epoch 424: w =2.164,loss=0.10195213\n",
      "epoch 425: w =2.164,loss=0.10179098\n",
      "epoch 426: w =2.165,loss=0.10163081\n",
      "epoch 427: w =2.165,loss=0.10147157\n",
      "epoch 428: w =2.165,loss=0.10131332\n",
      "epoch 429: w =2.166,loss=0.10115602\n",
      "epoch 430: w =2.166,loss=0.10099964\n",
      "epoch 431: w =2.167,loss=0.10084420\n",
      "epoch 432: w =2.167,loss=0.10068969\n",
      "epoch 433: w =2.167,loss=0.10053609\n",
      "epoch 434: w =2.168,loss=0.10038341\n",
      "epoch 435: w =2.168,loss=0.10023160\n",
      "epoch 436: w =2.169,loss=0.10008079\n",
      "epoch 437: w =2.169,loss=0.09993084\n",
      "epoch 438: w =2.169,loss=0.09978180\n",
      "epoch 439: w =2.170,loss=0.09963362\n",
      "epoch 440: w =2.170,loss=0.09948632\n",
      "epoch 441: w =2.171,loss=0.09933995\n",
      "epoch 442: w =2.171,loss=0.09919439\n",
      "epoch 443: w =2.171,loss=0.09904975\n",
      "epoch 444: w =2.172,loss=0.09890597\n",
      "epoch 445: w =2.172,loss=0.09876299\n",
      "epoch 446: w =2.172,loss=0.09862096\n",
      "epoch 447: w =2.173,loss=0.09847973\n",
      "epoch 448: w =2.173,loss=0.09833936\n",
      "epoch 449: w =2.174,loss=0.09819981\n",
      "epoch 450: w =2.174,loss=0.09806108\n",
      "epoch 451: w =2.174,loss=0.09792320\n",
      "epoch 452: w =2.175,loss=0.09778616\n",
      "epoch 453: w =2.175,loss=0.09764997\n",
      "epoch 454: w =2.175,loss=0.09751451\n",
      "epoch 455: w =2.176,loss=0.09737990\n",
      "epoch 456: w =2.176,loss=0.09724609\n",
      "epoch 457: w =2.177,loss=0.09711308\n",
      "epoch 458: w =2.177,loss=0.09698091\n",
      "epoch 459: w =2.177,loss=0.09684944\n",
      "epoch 460: w =2.178,loss=0.09671881\n",
      "epoch 461: w =2.178,loss=0.09658899\n",
      "epoch 462: w =2.178,loss=0.09645992\n",
      "epoch 463: w =2.179,loss=0.09633157\n",
      "epoch 464: w =2.179,loss=0.09620403\n",
      "epoch 465: w =2.180,loss=0.09607731\n",
      "epoch 466: w =2.180,loss=0.09595126\n",
      "epoch 467: w =2.180,loss=0.09582600\n",
      "epoch 468: w =2.181,loss=0.09570149\n",
      "epoch 469: w =2.181,loss=0.09557770\n",
      "epoch 470: w =2.181,loss=0.09545469\n",
      "epoch 471: w =2.182,loss=0.09533240\n",
      "epoch 472: w =2.182,loss=0.09521081\n",
      "epoch 473: w =2.182,loss=0.09509000\n",
      "epoch 474: w =2.183,loss=0.09496990\n",
      "epoch 475: w =2.183,loss=0.09485043\n",
      "epoch 476: w =2.183,loss=0.09473179\n",
      "epoch 477: w =2.184,loss=0.09461383\n",
      "epoch 478: w =2.184,loss=0.09449656\n",
      "epoch 479: w =2.184,loss=0.09438001\n",
      "epoch 480: w =2.185,loss=0.09426412\n",
      "epoch 481: w =2.185,loss=0.09414893\n",
      "epoch 482: w =2.186,loss=0.09403447\n",
      "epoch 483: w =2.186,loss=0.09392064\n",
      "epoch 484: w =2.186,loss=0.09380754\n",
      "epoch 485: w =2.187,loss=0.09369509\n",
      "epoch 486: w =2.187,loss=0.09358331\n",
      "epoch 487: w =2.187,loss=0.09347221\n",
      "epoch 488: w =2.188,loss=0.09336177\n",
      "epoch 489: w =2.188,loss=0.09325198\n",
      "epoch 490: w =2.188,loss=0.09314284\n",
      "epoch 491: w =2.189,loss=0.09303437\n",
      "epoch 492: w =2.189,loss=0.09292653\n",
      "epoch 493: w =2.189,loss=0.09281938\n",
      "epoch 494: w =2.190,loss=0.09271283\n",
      "epoch 495: w =2.190,loss=0.09260695\n",
      "epoch 496: w =2.190,loss=0.09250164\n",
      "epoch 497: w =2.191,loss=0.09239703\n",
      "epoch 498: w =2.191,loss=0.09229298\n",
      "epoch 499: w =2.191,loss=0.09218964\n",
      "epoch 500: w =2.192,loss=0.09208684\n",
      "prediction after training:f(5)=10.277\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_pred = model.forward(X)\n",
    "    l = loss(y,y_pred)\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if epoch%1 == 0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch+1}: w ={w[0][0].item():.3f},loss={l:.8f}')\n",
    "print(f\"prediction after training:f(5)={model(X_test).item():.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c9e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
